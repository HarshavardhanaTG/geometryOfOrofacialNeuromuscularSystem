{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Training set for NATO phonetic alphabets.\n",
    "\n",
    "Each alphabet is repeated 20 times. 16 instances of each alphabet are used for training and 4 instances for validation. \n",
    "Test sets are the entire RAINBOW and GRANDFATHER passages.\n",
    "\n",
    "0: Alfa\n",
    "1: Bravo\n",
    "2: Charlie\n",
    "3: Delta\n",
    "4: Echo\n",
    "5: Foxtrot\n",
    "6: Golf\n",
    "7: Hotel\n",
    "8: India\n",
    "9: Juliette\n",
    "10: Kilo\n",
    "11: Lima\n",
    "12: Mike\n",
    "13: November\n",
    "14: Oscar\n",
    "15: Papa\n",
    "16: Quebec\n",
    "17: Romeo\n",
    "18: Sierra\n",
    "19: Tango\n",
    "20: Uniform\n",
    "21: Victor\n",
    "22: Whiskey\n",
    "23: X-ray\n",
    "24: Yankee\n",
    "25: Zulu\n",
    "\n",
    "DATA is given in a numpy array of dimensions (520, 22, 7500) - (26 alphabets each repeated 20 times, 22 channels, 7500 time samples).\n",
    "Raw data was filtered using 3rd order Butterworth bandpass filter between 80 and 1000 Hertz.\n",
    "\n",
    "RUN this file before running Part2_NATOAlphabets_GrandfatherRnn.ipynb and Part2_NATOAlphabets_RainbowRnn.ipynb as this files saves the model weights for testing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from manifoldRnn import spdNN\n",
    "from manifoldRnn import optimizers \n",
    "from manifoldRnn import trainTest\n",
    "from manifoldRnn import spdRnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index].astype('float32'), self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = \"cuda:0\" \n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberAlphabets = 26\n",
    "trialsPerAlphabet = 20\n",
    "numberTrials = numberAlphabets * trialsPerAlphabet\n",
    "numberChannels = 22\n",
    "windowLength = 7500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectNumber = 4\n",
    "subject = \"Subject\" + str(subjectNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = np.load(\"Experiment2/\" + subject + \"/trainSet.npy\")\n",
    "   \n",
    "mean = np.mean(DATA, axis = -1)\n",
    "std = np.std(DATA, axis = -1)\n",
    "DATA = (DATA - mean[..., np.newaxis])/(std[..., np.newaxis] + 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicedMatrices = np.zeros((numberAlphabets * trialsPerAlphabet, 46, numberChannels, numberChannels))\n",
    "for j in range(numberAlphabets * trialsPerAlphabet):\n",
    "    for i in range(46):\n",
    "        where = i * 150 + 300\n",
    "        start = where - 300\n",
    "        End = where + 450\n",
    "        slicedMatrices[j, i] = 1/750 * DATA[j, :, start:End] @ DATA[j, :, start:End].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsByAlphabet = np.array([[i] * trialsPerAlphabet for i in range(numberAlphabets)]).reshape(numberTrials)\n",
    "\n",
    "Indices =  {}\n",
    "for i in range(numberAlphabets):\n",
    "    Indices[i] = []\n",
    "for i in range(len(labelsByAlphabet)):\n",
    "    Indices[labelsByAlphabet[i]].append(i)\n",
    "\n",
    "covariancesLabels = np.zeros((numberAlphabets, trialsPerAlphabet, 46, numberChannels, numberChannels))\n",
    "for i in range(numberAlphabets):\n",
    "    for j in range(trialsPerAlphabet):\n",
    "        covariancesLabels[i, j] = slicedMatrices[Indices[i][j]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures = np.zeros((numberAlphabets * 16, 46, numberChannels, numberChannels))\n",
    "trainLabels = np.zeros((numberAlphabets * 16))\n",
    "count = 0\n",
    "for i in range(numberAlphabets):\n",
    "    trainFeatures[count:count + 4] = covariancesLabels[i, :4]\n",
    "    trainFeatures[count + 4:count + 8] = covariancesLabels[i, 5:9]\n",
    "    trainFeatures[count + 8:count + 12] = covariancesLabels[i, 10:14]\n",
    "    trainFeatures[count + 12:count + 16] = covariancesLabels[i, 15:19]\n",
    "    trainLabels[count:count + 16] = [i] * 16\n",
    "    count += 16\n",
    "\n",
    "valFeatures = np.zeros((numberAlphabets * 4, 46, numberChannels, numberChannels))\n",
    "valLabels = np.zeros((numberAlphabets * 4))\n",
    "count = 0\n",
    "for i in range(numberAlphabets):\n",
    "    valFeatures[count] = covariancesLabels[i,4]\n",
    "    valFeatures[count + 1] = covariancesLabels[i, 9]\n",
    "    valFeatures[count + 2] = covariancesLabels[i, 14]\n",
    "    valFeatures[count + 3] = covariancesLabels[i, 19]\n",
    "    valLabels[count:count + 4] = [i] * 4\n",
    "    count += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = BaseDataset(trainFeatures, trainLabels)\n",
    "valDataset = BaseDataset(valFeatures, valLabels)\n",
    "trainDataloader = DataLoader(trainDataset, batch_size = 32, shuffle = True)\n",
    "valDataloader = DataLoader(valDataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145788\n"
     ]
    }
   ],
   "source": [
    "numberEpochs = 100\n",
    "\n",
    "model = spdRnn.spdRnnNet(numberAlphabets).to(device)\n",
    "numParams = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(numParams)\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "cnnOptimizer = optimizers.StiefelOptim(model.CNN.parameters(), lr = 0.05)\n",
    "rnnOptimizer = optim.Adam(model.RNN.parameters(), lr = 0.001, weight_decay = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Training loss: 0.1028, Training accuracy: 3.85%, Val loss: 0.1240, Val accuracy: 6.73%\n",
      "Epoch: 2/100, Training loss: 0.1008, Training accuracy: 7.21%, Val loss: 0.1225, Val accuracy: 13.46%\n",
      "Epoch: 3/100, Training loss: 0.0975, Training accuracy: 11.54%, Val loss: 0.1151, Val accuracy: 9.62%\n",
      "Epoch: 4/100, Training loss: 0.0917, Training accuracy: 14.42%, Val loss: 0.1095, Val accuracy: 24.04%\n",
      "Epoch: 5/100, Training loss: 0.0851, Training accuracy: 24.76%, Val loss: 0.0966, Val accuracy: 27.88%\n",
      "Epoch: 6/100, Training loss: 0.0776, Training accuracy: 23.32%, Val loss: 0.0858, Val accuracy: 26.92%\n",
      "Epoch: 7/100, Training loss: 0.0718, Training accuracy: 29.57%, Val loss: 0.0885, Val accuracy: 23.08%\n",
      "Epoch: 8/100, Training loss: 0.0646, Training accuracy: 32.45%, Val loss: 0.0863, Val accuracy: 25.96%\n",
      "Epoch: 9/100, Training loss: 0.0656, Training accuracy: 33.17%, Val loss: 0.0748, Val accuracy: 36.54%\n",
      "Epoch: 10/100, Training loss: 0.0552, Training accuracy: 38.70%, Val loss: 0.0680, Val accuracy: 35.58%\n",
      "Epoch: 11/100, Training loss: 0.0524, Training accuracy: 41.11%, Val loss: 0.0739, Val accuracy: 39.42%\n",
      "Epoch: 12/100, Training loss: 0.0500, Training accuracy: 48.08%, Val loss: 0.0655, Val accuracy: 50.96%\n",
      "Epoch: 13/100, Training loss: 0.0444, Training accuracy: 54.33%, Val loss: 0.0603, Val accuracy: 55.77%\n",
      "Epoch: 14/100, Training loss: 0.0452, Training accuracy: 51.92%, Val loss: 0.0548, Val accuracy: 54.81%\n",
      "Epoch: 15/100, Training loss: 0.0389, Training accuracy: 62.50%, Val loss: 0.0533, Val accuracy: 55.77%\n",
      "Epoch: 16/100, Training loss: 0.0404, Training accuracy: 61.06%, Val loss: 0.0513, Val accuracy: 59.62%\n",
      "Epoch: 17/100, Training loss: 0.0335, Training accuracy: 67.07%, Val loss: 0.0474, Val accuracy: 62.50%\n",
      "Epoch: 18/100, Training loss: 0.0308, Training accuracy: 65.87%, Val loss: 0.0465, Val accuracy: 63.46%\n",
      "Epoch: 19/100, Training loss: 0.0231, Training accuracy: 77.64%, Val loss: 0.0465, Val accuracy: 69.23%\n",
      "Epoch: 20/100, Training loss: 0.0254, Training accuracy: 76.92%, Val loss: 0.0385, Val accuracy: 76.92%\n",
      "Epoch: 21/100, Training loss: 0.0209, Training accuracy: 77.16%, Val loss: 0.0307, Val accuracy: 75.96%\n",
      "Epoch: 22/100, Training loss: 0.0219, Training accuracy: 75.96%, Val loss: 0.0395, Val accuracy: 70.19%\n",
      "Epoch: 23/100, Training loss: 0.0201, Training accuracy: 80.29%, Val loss: 0.0384, Val accuracy: 71.15%\n",
      "Epoch: 24/100, Training loss: 0.0184, Training accuracy: 81.73%, Val loss: 0.0324, Val accuracy: 69.23%\n",
      "Epoch: 25/100, Training loss: 0.0155, Training accuracy: 86.30%, Val loss: 0.0345, Val accuracy: 71.15%\n",
      "Epoch: 26/100, Training loss: 0.0150, Training accuracy: 84.62%, Val loss: 0.0302, Val accuracy: 75.00%\n",
      "Epoch: 27/100, Training loss: 0.0147, Training accuracy: 85.82%, Val loss: 0.0393, Val accuracy: 81.73%\n",
      "Epoch: 28/100, Training loss: 0.0123, Training accuracy: 87.26%, Val loss: 0.0347, Val accuracy: 76.92%\n",
      "Epoch: 29/100, Training loss: 0.0129, Training accuracy: 85.82%, Val loss: 0.0266, Val accuracy: 75.00%\n",
      "Epoch: 30/100, Training loss: 0.0127, Training accuracy: 88.22%, Val loss: 0.0305, Val accuracy: 78.85%\n",
      "Epoch: 31/100, Training loss: 0.0104, Training accuracy: 90.62%, Val loss: 0.0336, Val accuracy: 79.81%\n",
      "Epoch: 32/100, Training loss: 0.0094, Training accuracy: 91.83%, Val loss: 0.0342, Val accuracy: 73.08%\n",
      "Epoch: 33/100, Training loss: 0.0097, Training accuracy: 89.66%, Val loss: 0.0229, Val accuracy: 76.92%\n",
      "Epoch: 34/100, Training loss: 0.0089, Training accuracy: 90.38%, Val loss: 0.0284, Val accuracy: 75.00%\n",
      "Epoch: 35/100, Training loss: 0.0070, Training accuracy: 92.31%, Val loss: 0.0214, Val accuracy: 83.65%\n",
      "Epoch: 36/100, Training loss: 0.0047, Training accuracy: 97.60%, Val loss: 0.0229, Val accuracy: 81.73%\n",
      "Epoch: 37/100, Training loss: 0.0053, Training accuracy: 95.19%, Val loss: 0.0202, Val accuracy: 81.73%\n",
      "Epoch: 38/100, Training loss: 0.0076, Training accuracy: 92.31%, Val loss: 0.0342, Val accuracy: 82.69%\n",
      "Epoch: 39/100, Training loss: 0.0057, Training accuracy: 95.19%, Val loss: 0.0323, Val accuracy: 77.88%\n",
      "Epoch: 40/100, Training loss: 0.0061, Training accuracy: 94.95%, Val loss: 0.0272, Val accuracy: 84.62%\n",
      "Epoch: 41/100, Training loss: 0.0040, Training accuracy: 97.12%, Val loss: 0.0203, Val accuracy: 86.54%\n",
      "Epoch: 42/100, Training loss: 0.0027, Training accuracy: 98.32%, Val loss: 0.0332, Val accuracy: 81.73%\n",
      "Epoch: 43/100, Training loss: 0.0025, Training accuracy: 98.32%, Val loss: 0.0225, Val accuracy: 85.58%\n",
      "Epoch: 44/100, Training loss: 0.0032, Training accuracy: 97.12%, Val loss: 0.0252, Val accuracy: 82.69%\n",
      "Epoch: 45/100, Training loss: 0.0035, Training accuracy: 96.39%, Val loss: 0.0178, Val accuracy: 85.58%\n",
      "Epoch: 46/100, Training loss: 0.0051, Training accuracy: 95.43%, Val loss: 0.0293, Val accuracy: 78.85%\n",
      "Epoch: 47/100, Training loss: 0.0126, Training accuracy: 85.82%, Val loss: 0.0344, Val accuracy: 75.96%\n",
      "Epoch: 48/100, Training loss: 0.0075, Training accuracy: 92.55%, Val loss: 0.0572, Val accuracy: 75.96%\n",
      "Epoch: 49/100, Training loss: 0.0072, Training accuracy: 94.71%, Val loss: 0.0352, Val accuracy: 76.92%\n",
      "Epoch: 50/100, Training loss: 0.0061, Training accuracy: 95.19%, Val loss: 0.0239, Val accuracy: 84.62%\n",
      "Epoch: 51/100, Training loss: 0.0046, Training accuracy: 95.91%, Val loss: 0.0259, Val accuracy: 82.69%\n",
      "Epoch: 52/100, Training loss: 0.0042, Training accuracy: 97.12%, Val loss: 0.0274, Val accuracy: 84.62%\n",
      "Epoch: 53/100, Training loss: 0.0033, Training accuracy: 97.12%, Val loss: 0.0328, Val accuracy: 89.42%\n",
      "Epoch: 54/100, Training loss: 0.0035, Training accuracy: 96.39%, Val loss: 0.0277, Val accuracy: 87.50%\n",
      "Epoch: 55/100, Training loss: 0.0032, Training accuracy: 97.12%, Val loss: 0.0237, Val accuracy: 85.58%\n",
      "Epoch: 56/100, Training loss: 0.0019, Training accuracy: 99.04%, Val loss: 0.0222, Val accuracy: 87.50%\n",
      "Epoch: 57/100, Training loss: 0.0017, Training accuracy: 98.80%, Val loss: 0.0205, Val accuracy: 88.46%\n",
      "Epoch: 58/100, Training loss: 0.0010, Training accuracy: 99.52%, Val loss: 0.0213, Val accuracy: 91.35%\n",
      "Epoch: 59/100, Training loss: 0.0007, Training accuracy: 99.76%, Val loss: 0.0195, Val accuracy: 90.38%\n",
      "Epoch: 60/100, Training loss: 0.0011, Training accuracy: 98.80%, Val loss: 0.0203, Val accuracy: 88.46%\n",
      "Epoch: 61/100, Training loss: 0.0011, Training accuracy: 99.52%, Val loss: 0.0236, Val accuracy: 87.50%\n",
      "Epoch: 62/100, Training loss: 0.0006, Training accuracy: 99.76%, Val loss: 0.0180, Val accuracy: 89.42%\n",
      "Epoch: 63/100, Training loss: 0.0005, Training accuracy: 99.76%, Val loss: 0.0196, Val accuracy: 88.46%\n",
      "Epoch: 64/100, Training loss: 0.0003, Training accuracy: 100.00%, Val loss: 0.0199, Val accuracy: 89.42%\n",
      "Epoch: 65/100, Training loss: 0.0002, Training accuracy: 100.00%, Val loss: 0.0206, Val accuracy: 89.42%\n",
      "Epoch: 66/100, Training loss: 0.0002, Training accuracy: 100.00%, Val loss: 0.0190, Val accuracy: 90.38%\n",
      "Epoch: 67/100, Training loss: 0.0003, Training accuracy: 99.76%, Val loss: 0.0196, Val accuracy: 89.42%\n",
      "Epoch: 68/100, Training loss: 0.0003, Training accuracy: 99.76%, Val loss: 0.0220, Val accuracy: 88.46%\n",
      "Epoch: 69/100, Training loss: 0.0002, Training accuracy: 100.00%, Val loss: 0.0214, Val accuracy: 87.50%\n",
      "Epoch: 70/100, Training loss: 0.0002, Training accuracy: 100.00%, Val loss: 0.0206, Val accuracy: 89.42%\n",
      "Epoch: 71/100, Training loss: 0.0002, Training accuracy: 100.00%, Val loss: 0.0224, Val accuracy: 88.46%\n",
      "Epoch: 72/100, Training loss: 0.0001, Training accuracy: 100.00%, Val loss: 0.0219, Val accuracy: 88.46%\n",
      "Epoch: 73/100, Training loss: 0.0001, Training accuracy: 100.00%, Val loss: 0.0222, Val accuracy: 89.42%\n",
      "Epoch: 74/100, Training loss: 0.0001, Training accuracy: 100.00%, Val loss: 0.0201, Val accuracy: 89.42%\n",
      "Epoch: 75/100, Training loss: 0.0001, Training accuracy: 100.00%, Val loss: 0.0223, Val accuracy: 90.38%\n",
      "Epoch: 76/100, Training loss: 0.0001, Training accuracy: 100.00%, Val loss: 0.0242, Val accuracy: 88.46%\n",
      "Epoch: 77/100, Training loss: 0.0001, Training accuracy: 100.00%, Val loss: 0.0228, Val accuracy: 90.38%\n",
      "Epoch: 78/100, Training loss: 0.0001, Training accuracy: 100.00%, Val loss: 0.0232, Val accuracy: 89.42%\n",
      "Epoch: 79/100, Training loss: 0.0001, Training accuracy: 100.00%, Val loss: 0.0232, Val accuracy: 89.42%\n",
      "Epoch: 80/100, Training loss: 0.0001, Training accuracy: 100.00%, Val loss: 0.0186, Val accuracy: 90.38%\n",
      "Epoch: 81/100, Training loss: 0.0002, Training accuracy: 100.00%, Val loss: 0.0207, Val accuracy: 89.42%\n",
      "Epoch: 82/100, Training loss: 0.0002, Training accuracy: 100.00%, Val loss: 0.0219, Val accuracy: 88.46%\n",
      "Epoch: 83/100, Training loss: 0.0012, Training accuracy: 99.76%, Val loss: 0.0305, Val accuracy: 86.54%\n",
      "Epoch: 84/100, Training loss: 0.0002, Training accuracy: 100.00%, Val loss: 0.0180, Val accuracy: 88.46%\n",
      "Epoch: 85/100, Training loss: 0.0002, Training accuracy: 100.00%, Val loss: 0.0201, Val accuracy: 91.35%\n",
      "Epoch: 86/100, Training loss: 0.0009, Training accuracy: 99.28%, Val loss: 0.0385, Val accuracy: 74.04%\n",
      "Epoch: 87/100, Training loss: 0.0173, Training accuracy: 85.34%, Val loss: 0.0428, Val accuracy: 66.35%\n",
      "Epoch: 88/100, Training loss: 0.0268, Training accuracy: 71.88%, Val loss: 0.0372, Val accuracy: 70.19%\n",
      "Epoch: 89/100, Training loss: 0.0218, Training accuracy: 80.05%, Val loss: 0.0423, Val accuracy: 71.15%\n",
      "Epoch: 90/100, Training loss: 0.0156, Training accuracy: 86.30%, Val loss: 0.0382, Val accuracy: 74.04%\n",
      "Epoch: 91/100, Training loss: 0.0106, Training accuracy: 90.87%, Val loss: 0.0340, Val accuracy: 83.65%\n",
      "Epoch: 92/100, Training loss: 0.0063, Training accuracy: 95.43%, Val loss: 0.0287, Val accuracy: 83.65%\n",
      "Epoch: 93/100, Training loss: 0.0047, Training accuracy: 96.63%, Val loss: 0.0278, Val accuracy: 81.73%\n",
      "Epoch: 94/100, Training loss: 0.0040, Training accuracy: 97.12%, Val loss: 0.0288, Val accuracy: 84.62%\n",
      "Epoch: 95/100, Training loss: 0.0036, Training accuracy: 97.36%, Val loss: 0.0253, Val accuracy: 83.65%\n",
      "Epoch: 96/100, Training loss: 0.0032, Training accuracy: 97.84%, Val loss: 0.0224, Val accuracy: 83.65%\n",
      "Epoch: 97/100, Training loss: 0.0019, Training accuracy: 98.80%, Val loss: 0.0255, Val accuracy: 83.65%\n",
      "Epoch: 98/100, Training loss: 0.0014, Training accuracy: 99.28%, Val loss: 0.0300, Val accuracy: 86.54%\n",
      "Epoch: 99/100, Training loss: 0.0007, Training accuracy: 99.76%, Val loss: 0.0295, Val accuracy: 90.38%\n",
      "Epoch: 100/100, Training loss: 0.0007, Training accuracy: 99.76%, Val loss: 0.0284, Val accuracy: 87.50%\n",
      "91.34615384615384\n"
     ]
    }
   ],
   "source": [
    "maxValue = 0\n",
    "for epoch in range(numberEpochs):\n",
    "    trainLoss, trainAccuracy = trainTest.trainOperation(model, device, trainDataloader, cnnOptimizer, rnnOptimizer, lossFunction)\n",
    "    valLoss, valAccuracy = trainTest.testOperation(model, device, valDataloader, lossFunction)\n",
    "    if maxValue < valAccuracy:\n",
    "        maxValue = valAccuracy\n",
    "        torch.save(model.state_dict(), 'Experiment2/' + subject + '/rnn.pt')\n",
    "    print(f'Epoch: {epoch + 1}/{numberEpochs}, Training loss: {trainLoss:.4f}, Training accuracy: {trainAccuracy:.2f}%, Val loss: {valLoss:.4f}, Val accuracy: {valAccuracy:.2f}%')\n",
    "print(maxValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emgSpeech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
